# Gradient-Descent-Optimization
Final Project - CSE 569 - FSL

## Project Description


In this project, we have done evaluation of Gradient Descent Optimization techniques for neural networks. We have implemented a 3-layer fully-connected neural network using Numpy library to classify the fashion-MNIST dataset and all the techniques have been teste. Following are the techniques used:
* No Momentum (regular network without momentum)
* Polyak’s classical momentum
* Nesterov’s Accelerated Gradient
* RmsProp 
* ADAM

All the above techniques has been evaluated by keeping either of learning rates or batch sizes as constant.

The Fashion MNIST dataset can be downloaded from: https://arxiv.org/abs/1708.07747